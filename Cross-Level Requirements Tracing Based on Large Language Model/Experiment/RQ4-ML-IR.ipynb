{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anconda\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"CCHIT.csv\"\n",
    "save_path = \"IR_dataset/IR_CCHIT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_as_list(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        first_row = True\n",
    "        for row in csv_reader:\n",
    "            if first_row:  \n",
    "                first_row = False\n",
    "                continue\n",
    "            data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = read_csv_as_list(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_column = list({row[0] for row in csv_data})\n",
    "low_column = list({row[1] for row in csv_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_count_key(text):\n",
    "    words = text.split()\n",
    "    table = {}\n",
    "    for word in words:\n",
    "        if word != \"\":\n",
    "            if word in table:\n",
    "                table[word] += 1\n",
    "            else:\n",
    "                table[word] = 1\n",
    "    return sorted(table.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "\n",
    "def vsm_merge_keys(dic1, dic2):\n",
    "    arrayKey = []\n",
    "    for item in dic1:\n",
    "        arrayKey.append(item[0])\n",
    "    for item in dic2:\n",
    "        if item[0] not in arrayKey:\n",
    "            arrayKey.append(item[0])\n",
    "    \n",
    "    arrayNum1 = [0] * len(arrayKey)\n",
    "    arrayNum2 = [0] * len(arrayKey)\n",
    "    \n",
    "    for i, key in enumerate(arrayKey):\n",
    "        for k, v in dic1:\n",
    "            if key == k:\n",
    "                arrayNum1[i] = v\n",
    "        for k, v in dic2:\n",
    "            if key == k:\n",
    "                arrayNum2[i] = v\n",
    "\n",
    "    dot_product = sum(a * b for a, b in zip(arrayNum1, arrayNum2))\n",
    "    magnitude1 = math.sqrt(sum(a * a for a in arrayNum1))\n",
    "    magnitude2 = math.sqrt(sum(b * b for b in arrayNum2))\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def compute_similarity(text1, text2):\n",
    "\n",
    "    dic1 = vsm_count_key(text1)\n",
    "    dic2 = vsm_count_key(text2)\n",
    "\n",
    "    result = vsm_merge_keys(dic1, dic2)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The system shall allow a user to foward notification to someone else at the site'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csv_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = compute_similarity(text1, text2)\n",
    "    tfidf_dataset.append(result)\n",
    "\n",
    "# print(tfidf_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tfidf_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_idf(docs):\n",
    "    D = len(docs)\n",
    "    df = {}\n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            df[word] = df.get(word, 0) + 1\n",
    "    \n",
    "    idf = {}\n",
    "    for word, freq in df.items():\n",
    "        idf[word] = math.log((D - freq + 0.5) / (freq + 0.5) + 1)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "def compute_score(doc1, doc2, idf):\n",
    "    avgdl = (len(doc1) + len(doc2)) / 2\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "    score = 0\n",
    "    doc1_counter = Counter(doc1)\n",
    "    doc2_counter = Counter(doc2)\n",
    "    for word in set(doc1 + doc2):\n",
    "        if word not in idf:\n",
    "            continue\n",
    "        idf_val = idf[word]\n",
    "        tf1 = doc1_counter[word]\n",
    "        tf2 = doc2_counter[word]\n",
    "        doc1_len = len(doc1)\n",
    "        doc2_len = len(doc2)\n",
    "        score += (idf_val * ((tf1 * (k1 + 1)) / (tf1 + k1 * (1 - b + b * doc1_len / avgdl))) *\n",
    "                              ((tf2 * (k1 + 1)) / (tf2 + k1 * (1 - b + b * doc2_len / avgdl))))\n",
    "    return score\n",
    "\n",
    "def compute_bm25_similarity(doc1, doc2):\n",
    "    idf = compute_idf([doc1, doc2])\n",
    "    score = compute_score(doc1, doc2, idf)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "BM25_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = compute_bm25_similarity(text1, text2)\n",
    "    BM25_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[10.261973224867328,\n",
       " 11.117247995112578,\n",
       " 8.92132304247616,\n",
       " 9.435237413878538,\n",
       " 10.179668920536955,\n",
       " 11.770119223844231,\n",
       " 10.776218185874926,\n",
       " 12.87633104840782,\n",
       " 11.234692953218097,\n",
       " 13.77907790462664,\n",
       " 9.893233108486474,\n",
       " 11.938213560823142,\n",
       " 12.470807174483172,\n",
       " 10.29327931419529,\n",
       " 11.16533849510064,\n",
       " 8.25131761803893,\n",
       " 10.317576391126572,\n",
       " 5.514574661582996,\n",
       " 9.02465251926342,\n",
       " 9.343025369793114,\n",
       " 10.381412703137178,\n",
       " 9.719739306197022,\n",
       " 8.330380091326347,\n",
       " 12.760185441985012,\n",
       " 12.318475071863237,\n",
       " 13.868302118090623,\n",
       " 7.805861588815944,\n",
       " 9.977986640991771,\n",
       " 10.465548644626411,\n",
       " 12.062766068900348,\n",
       " 10.842160602460178,\n",
       " 10.54105542501929,\n",
       " 13.111962448135879,\n",
       " 11.653010728469319,\n",
       " 9.223411787734813,\n",
       " 9.15889356663311,\n",
       " 10.48066789969187,\n",
       " 10.750733623016487,\n",
       " 9.560115831942618,\n",
       " 9.92345992176589,\n",
       " 10.113779949934921,\n",
       " 11.470401873779645,\n",
       " 10.651242030629597,\n",
       " 11.513133513995259,\n",
       " 11.243838216534103,\n",
       " 12.219092008512279,\n",
       " 10.248771015953466,\n",
       " 9.57753051608943,\n",
       " 13.451181285199485,\n",
       " 14.744339913668846,\n",
       " 14.189421235817322,\n",
       " 12.534284274441408,\n",
       " 12.20078408843128,\n",
       " 11.891046159884759,\n",
       " 13.333624477086289,\n",
       " 11.391297936215038,\n",
       " 11.604496109909107,\n",
       " 13.047597263504334,\n",
       " 11.567591057799216,\n",
       " 11.524883723410817,\n",
       " 12.715288601967632,\n",
       " 12.391377594462902,\n",
       " 10.407706083676494,\n",
       " 10.234158658549214,\n",
       " 10.110047440928925,\n",
       " 12.424847315190497,\n",
       " 9.774005807159405,\n",
       " 9.658605218268601,\n",
       " 14.451030017237589,\n",
       " 12.660683214254776,\n",
       " 12.660156752957576,\n",
       " 11.167141267831935,\n",
       " 10.704444394321147,\n",
       " 10.379141923893444,\n",
       " 7.430266144565782,\n",
       " 12.327990183339274,\n",
       " 9.126700715367518,\n",
       " 10.954098838648655,\n",
       " 10.388956026045149,\n",
       " 7.206514097684172,\n",
       " 12.231798228367147,\n",
       " 12.986751372192753,\n",
       " 13.231726945140432,\n",
       " 13.642087478644571,\n",
       " 7.411983065005823,\n",
       " 6.884547464660962,\n",
       " 9.498113973216608,\n",
       " 8.251800777381005,\n",
       " 6.545243563629114,\n",
       " 12.837826316381106,\n",
       " 11.57173392419334,\n",
       " 10.945341688264849,\n",
       " 12.021052836067353,\n",
       " 9.410702827398028,\n",
       " 9.987125929373446,\n",
       " 9.330319119625802,\n",
       " 9.56911573011452,\n",
       " 16.057943125373374,\n",
       " 15.723521865395242,\n",
       " 14.844033390293223,\n",
       " 10.575190675569628,\n",
       " 9.81052518748906,\n",
       " 12.739761054127749,\n",
       " 12.276387264001638,\n",
       " 8.230624168742468,\n",
       " 11.493516560725714,\n",
       " 12.341044804696853,\n",
       " 9.170189055334166,\n",
       " 8.571193934101132,\n",
       " 9.100591611772577,\n",
       " 11.549393079936069,\n",
       " 10.231834285326768,\n",
       " 10.877965424866503,\n",
       " 10.116896090899912,\n",
       " 11.003826276429029,\n",
       " 13.008071302570189,\n",
       " 10.790209353955483,\n",
       " 7.32548982476494,\n",
       " 13.26935671218073,\n",
       " 10.480268314690296,\n",
       " 10.24336927644265,\n",
       " 15.845230364279969,\n",
       " 10.098193340669173,\n",
       " 6.99186584607511,\n",
       " 10.340300651924798,\n",
       " 11.317961207244675,\n",
       " 8.666764498992535,\n",
       " 10.737533243272585,\n",
       " 9.725878133128132,\n",
       " 11.2277628267359,\n",
       " 12.02683690597602,\n",
       " 7.365644844334616,\n",
       " 11.562024598872181,\n",
       " 9.557653615491395,\n",
       " 11.887615614033349,\n",
       " 8.7989171704018,\n",
       " 9.346384003654878,\n",
       " 11.689361356494127,\n",
       " 12.299259672355975,\n",
       " 12.06328094219688,\n",
       " 10.613330969342135,\n",
       " 14.127580927748486,\n",
       " 10.988204027239282,\n",
       " 11.65852190107814,\n",
       " 8.191217129736152,\n",
       " 12.351257675400092,\n",
       " 10.329569309906061,\n",
       " 9.988168620995163,\n",
       " 10.283239107086892,\n",
       " 11.716712629894616,\n",
       " 12.71799758677331,\n",
       " 10.100500687300833,\n",
       " 10.746379859213718,\n",
       " 11.4422480692118,\n",
       " 13.316469844987605,\n",
       " 8.839135837810973,\n",
       " 9.537790541947425,\n",
       " 11.645160585974546,\n",
       " 6.99186584607511,\n",
       " 11.953977031039017,\n",
       " 10.389982074726106,\n",
       " 9.200850035480196,\n",
       " 11.080301907126024,\n",
       " 11.457229727735712,\n",
       " 11.994345841750668,\n",
       " 14.444151287669715,\n",
       " 12.22153522882344,\n",
       " 8.95611379511263,\n",
       " 11.246845885273872,\n",
       " 12.454621710388066,\n",
       " 9.505779438488636,\n",
       " 16.154056766961663,\n",
       " 12.31100774902828,\n",
       " 11.065803457079973,\n",
       " 10.877965424866503,\n",
       " 14.007673520532109,\n",
       " 15.121029866489891,\n",
       " 8.966958778091525,\n",
       " 12.7923576338789,\n",
       " 8.931420139709003,\n",
       " 13.396125885901053,\n",
       " 10.463819414999763,\n",
       " 9.769360297579304,\n",
       " 10.395654841505198,\n",
       " 9.742168051651142,\n",
       " 11.727503401313163,\n",
       " 9.281762775274787,\n",
       " 11.616601370422021,\n",
       " 14.424931827623105,\n",
       " 10.489669777949427,\n",
       " 12.657338965860562,\n",
       " 10.561301444676202,\n",
       " 14.321710630259762,\n",
       " 10.491495934772196,\n",
       " 10.21591405322203,\n",
       " 10.711677143135601,\n",
       " 8.71209397662196,\n",
       " 13.60655463586017,\n",
       " 10.457766076651934,\n",
       " 11.949268652326232,\n",
       " 8.922985712094434,\n",
       " 10.37464675184241,\n",
       " 12.570657403290172,\n",
       " 12.01471446212331,\n",
       " 10.783030695620987,\n",
       " 7.978191187589354,\n",
       " 10.93001137432957,\n",
       " 13.53261073035753,\n",
       " 11.226166537844067,\n",
       " 11.00288174559283,\n",
       " 10.971231410889997,\n",
       " 8.930578861012728,\n",
       " 11.443585763148407,\n",
       " 11.09076911668746,\n",
       " 10.20036456849098,\n",
       " 5.912393691815568,\n",
       " 12.726252119015333,\n",
       " 11.710101055726522,\n",
       " 10.480268314690296,\n",
       " 11.812095427737233,\n",
       " 9.742168051651142,\n",
       " 5.248179900507708,\n",
       " 9.940122787011388,\n",
       " 10.351710627343984,\n",
       " 14.477882671226531,\n",
       " 10.767798180645142,\n",
       " 12.457646645254917,\n",
       " 10.795643953286257,\n",
       " 12.455875338663557,\n",
       " 8.513855335885756,\n",
       " 10.790208059439022,\n",
       " 11.14581753236619,\n",
       " 8.237334692949036,\n",
       " 12.50124694352053,\n",
       " 7.531805162250595,\n",
       " 11.345770024292273,\n",
       " 10.741854526272492,\n",
       " 11.973628194687972,\n",
       " 12.377823052271252,\n",
       " 11.700006740296871,\n",
       " 11.65852252644995,\n",
       " 10.313449901235192,\n",
       " 10.93001137432957,\n",
       " 11.328683655279146,\n",
       " 8.512942401810255,\n",
       " 13.967245370635439,\n",
       " 8.84909633623142,\n",
       " 7.540439448944505,\n",
       " 13.095249572286757,\n",
       " 8.574809809628949,\n",
       " 15.622318097821859,\n",
       " 10.773433440909159]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BM25_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen Shannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "def compute_js_similarity(text1, text2):\n",
    "    counter1 = count_words(text1)\n",
    "    counter2 = count_words(text2)\n",
    "\n",
    "    all_words = list(set(counter1.keys()).union(set(counter2.keys())))\n",
    "\n",
    "    vector1 = np.array([counter1.get(word, 0) for word in all_words], dtype=float)\n",
    "    vector2 = np.array([counter2.get(word, 0) for word in all_words], dtype=float)\n",
    "\n",
    "    vector1 /= vector1.sum()\n",
    "    vector2 /= vector2.sum()\n",
    "    \n",
    "    js_distance = jensenshannon(vector1, vector2)\n",
    "    js_similarity = 1 - js_distance\n",
    "    return js_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Jensen_Shannon_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = compute_js_similarity(text1, text2)\n",
    "    Jensen_Shannon_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.4261900807736465,\n",
       " 0.326230547940511,\n",
       " 0.3362903799211808,\n",
       " 0.32022200655412736,\n",
       " 0.3930927254655858,\n",
       " 0.3543487912523329,\n",
       " 0.30790105186628725,\n",
       " 0.2650750786541355,\n",
       " 0.3032613031624798,\n",
       " 0.3169310406882899,\n",
       " 0.33029334840853874,\n",
       " 0.33103096988758296,\n",
       " 0.33739693039213936,\n",
       " 0.3857624948041001,\n",
       " 0.3553412839891944,\n",
       " 0.2012741582255282,\n",
       " 0.2601833334841108,\n",
       " 0.1674453888423022,\n",
       " 0.20484203002461543,\n",
       " 0.26334276608598395,\n",
       " 0.30636038189380943,\n",
       " 0.1941042012070664,\n",
       " 0.320811036265791,\n",
       " 0.37400399327018274,\n",
       " 0.30328672969093573,\n",
       " 0.29779285073090866,\n",
       " 0.3803109884053536,\n",
       " 0.3866205072425636,\n",
       " 0.34805814461048257,\n",
       " 0.3652072503200373,\n",
       " 0.34825636028823137,\n",
       " 0.25726594084430465,\n",
       " 0.3272034283002051,\n",
       " 0.28962678398139097,\n",
       " 0.29645196927295103,\n",
       " 0.27993230624427,\n",
       " 0.284131640619551,\n",
       " 0.2678020828059592,\n",
       " 0.29030313514933426,\n",
       " 0.2799323062442699,\n",
       " 0.2714797118670147,\n",
       " 0.284131640619551,\n",
       " 0.29134427958881093,\n",
       " 0.30497610087556726,\n",
       " 0.327603627868151,\n",
       " 0.31715378993729937,\n",
       " 0.32216331320921476,\n",
       " 0.3362903799211808,\n",
       " 0.35400491192147854,\n",
       " 0.3762977346047892,\n",
       " 0.40809072819883363,\n",
       " 0.32886951671667575,\n",
       " 0.3782866191285067,\n",
       " 0.5120706976931062,\n",
       " 0.3730920197387706,\n",
       " 0.44616946571973104,\n",
       " 0.4619688801214813,\n",
       " 0.4121204841559437,\n",
       " 0.42104747707672463,\n",
       " 0.29906324589244204,\n",
       " 0.39460332337978454,\n",
       " 0.3463795627787468,\n",
       " 0.3883915587503983,\n",
       " 0.33389132810682776,\n",
       " 0.2593725626351523,\n",
       " 0.4014945846333987,\n",
       " 0.19295644411444168,\n",
       " 0.34318630072607714,\n",
       " 0.40205365053780273,\n",
       " 0.3755483073088738,\n",
       " 0.37342957090256745,\n",
       " 0.3390112107043549,\n",
       " 0.3614104991187119,\n",
       " 0.3419038257431286,\n",
       " 0.3608830153918111,\n",
       " 0.3602293006378522,\n",
       " 0.3935257879518723,\n",
       " 0.41424505599052597,\n",
       " 0.36898660578118325,\n",
       " 0.1674453888423022,\n",
       " 0.3128216933323136,\n",
       " 0.3557086868261977,\n",
       " 0.3292796545710627,\n",
       " 0.29536493860143254,\n",
       " 0.2184118713306299,\n",
       " 0.30593068473383855,\n",
       " 0.323829051077015,\n",
       " 0.3044765823100525,\n",
       " 0.1674453888423022,\n",
       " 0.35463371919871656,\n",
       " 0.3096821843274199,\n",
       " 0.29172958750395384,\n",
       " 0.3078971459393297,\n",
       " 0.3730463491611792,\n",
       " 0.2158622435195202,\n",
       " 0.4205794832552252,\n",
       " 0.30752741509873527,\n",
       " 0.23590479700197597,\n",
       " 0.23821685348448618,\n",
       " 0.2413217269223744,\n",
       " 0.24969932648206905,\n",
       " 0.2550744848748401,\n",
       " 0.3228387346599513,\n",
       " 0.3203684381697838,\n",
       " 0.1674453888423023,\n",
       " 0.30497610087556726,\n",
       " 0.3136738949785316,\n",
       " 0.2885230184471206,\n",
       " 0.22500786534517359,\n",
       " 0.2786837404419318,\n",
       " 0.358251147170208,\n",
       " 0.3391186652608753,\n",
       " 0.2822098184122246,\n",
       " 0.3554371960588584,\n",
       " 0.3536814279231473,\n",
       " 0.3321683970148608,\n",
       " 0.2982145768229313,\n",
       " 0.23889782792201097,\n",
       " 0.31368923689187944,\n",
       " 0.3043502524618321,\n",
       " 0.29456910479818,\n",
       " 0.26444374045508146,\n",
       " 0.31764298764882715,\n",
       " 0.3575627247431685,\n",
       " 0.3314725363811982,\n",
       " 0.262598459008447,\n",
       " 0.1674453888423022,\n",
       " 0.3117410752122769,\n",
       " 0.3293000962196325,\n",
       " 0.23368792319027964,\n",
       " 0.2453612366870298,\n",
       " 0.2440865037116986,\n",
       " 0.2958764066111085,\n",
       " 0.38867974381401904,\n",
       " 0.3113978311747754,\n",
       " 0.36140986230178174,\n",
       " 0.301045177813291,\n",
       " 0.2593725626351524,\n",
       " 0.30100134331430983,\n",
       " 0.21575500279688364,\n",
       " 0.33716053347100716,\n",
       " 0.26663844657884284,\n",
       " 0.35786380586603583,\n",
       " 0.3025081358676941,\n",
       " 0.1674453888423023,\n",
       " 0.29279092763388403,\n",
       " 0.33716053347100716,\n",
       " 0.3817724158244049,\n",
       " 0.2734834326357166,\n",
       " 0.26667765224433604,\n",
       " 0.3069465865660276,\n",
       " 0.2742468593500129,\n",
       " 0.30032143230188224,\n",
       " 0.3021040854248047,\n",
       " 0.2869313859630076,\n",
       " 0.2633839550087109,\n",
       " 0.2786837404419318,\n",
       " 0.2679788089514177,\n",
       " 0.3575627247431685,\n",
       " 0.3156777712019635,\n",
       " 0.24445115746921908,\n",
       " 0.23979356462304036,\n",
       " 0.27791671106624005,\n",
       " 0.2963949551444386,\n",
       " 0.25683654641936493,\n",
       " 0.3074572069433843,\n",
       " 0.3249703223339293,\n",
       " 0.27053393816653526,\n",
       " 0.2825990834802663,\n",
       " 0.2644947970436107,\n",
       " 0.37584900187180514,\n",
       " 0.3098922516225683,\n",
       " 0.26396224975365457,\n",
       " 0.2976141391788588,\n",
       " 0.2822098184122246,\n",
       " 0.3610648122933855,\n",
       " 0.27028669514137904,\n",
       " 0.3895683639642,\n",
       " 0.36195837438259104,\n",
       " 0.20016254424419544,\n",
       " 0.3440183670239607,\n",
       " 0.33145871826462747,\n",
       " 0.28297310320530566,\n",
       " 0.30558273757859467,\n",
       " 0.36412591236422065,\n",
       " 0.3264996938112297,\n",
       " 0.22968570561824042,\n",
       " 0.2849018014424949,\n",
       " 0.2997602028495284,\n",
       " 0.33185950208404746,\n",
       " 0.28602640226659193,\n",
       " 0.3396863569901176,\n",
       " 0.288735792624482,\n",
       " 0.26677893061326297,\n",
       " 0.272983924065031,\n",
       " 0.35790078003326675,\n",
       " 0.3197938046319032,\n",
       " 0.37481703831363755,\n",
       " 0.3753664722103157,\n",
       " 0.2865976586051918,\n",
       " 0.35588933948751866,\n",
       " 0.2727509404283086,\n",
       " 0.2672427068146732,\n",
       " 0.23479811215618096,\n",
       " 0.3246529169509188,\n",
       " 0.22316399344315951,\n",
       " 0.25302020248198553,\n",
       " 0.27297965685858894,\n",
       " 0.30933140634791634,\n",
       " 0.3859230284466991,\n",
       " 0.284131640619551,\n",
       " 0.2859420123978813,\n",
       " 0.3437451018612223,\n",
       " 0.4231452802126042,\n",
       " 0.33237414073964056,\n",
       " 0.1674453888423022,\n",
       " 0.2723608944551603,\n",
       " 0.3426758017557203,\n",
       " 0.3043502524618321,\n",
       " 0.26584622404850744,\n",
       " 0.36412591236422065,\n",
       " 0.1674453888423022,\n",
       " 0.35867164682853503,\n",
       " 0.2591627637092512,\n",
       " 0.2804743917568765,\n",
       " 0.2934064649770979,\n",
       " 0.27613111330512785,\n",
       " 0.2711455104324666,\n",
       " 0.2807978957182642,\n",
       " 0.1674453888423022,\n",
       " 0.29096286494591095,\n",
       " 0.4089830460122621,\n",
       " 0.31145851788725676,\n",
       " 0.3264996938112297,\n",
       " 0.33494269023721224,\n",
       " 0.2862272925534637,\n",
       " 0.26677893061326297,\n",
       " 0.2757211841364404,\n",
       " 0.2864997258225135,\n",
       " 0.2894841209515393,\n",
       " 0.3269606139571257,\n",
       " 0.2957540575132319,\n",
       " 0.25302020248198553,\n",
       " 0.3436297780939682,\n",
       " 0.31245651803319985,\n",
       " 0.36554447990872674,\n",
       " 0.29124604688102185,\n",
       " 0.33494269023721224,\n",
       " 0.3162955294602815,\n",
       " 0.32774604874185964,\n",
       " 0.2725005245513604,\n",
       " 0.26184458605213456]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Jensen_Shannon_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_lsa_similarity_measure(text1, text2, n=1):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    \n",
    "    svd_model = TruncatedSVD(n_components=n)  \n",
    "    X2 = svd_model.fit_transform(X)  \n",
    "    cosine_sim = cosine_similarity(X)\n",
    "    return cosine_sim[0, 1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lsa_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = cosine_lsa_similarity_measure(text1, text2,1)\n",
    "    lsa_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lsa_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Preprocess text: lowercase, tokenize, remove stopwords. \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w.lower() for w in nltk.word_tokenize(text) if w.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "def lda_similarity(text1, text2):\n",
    "    \"\"\" Calculate LDA similarity between two text strings. \"\"\"\n",
    "    processed_text1 = preprocess_text(text1)\n",
    "    processed_text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = Dictionary([processed_text1, processed_text2])\n",
    "    corpus = [dictionary.doc2bow(processed_text1), dictionary.doc2bow(processed_text2)]\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "    \n",
    "    # Get document topics\n",
    "    doc_bow1 = dictionary.doc2bow(processed_text1)\n",
    "    doc_bow2 = dictionary.doc2bow(processed_text2)\n",
    "    \n",
    "    doc_lda1 = lda_model.get_document_topics(doc_bow1)\n",
    "    doc_lda2 = lda_model.get_document_topics(doc_bow2)\n",
    "    \n",
    "    # Create dense topic distribution vectors\n",
    "    vec1 = np.zeros(lda_model.num_topics)\n",
    "    vec2 = np.zeros(lda_model.num_topics)\n",
    "    for topic, prob in doc_lda1:\n",
    "        vec1[topic] = prob\n",
    "    for topic, prob in doc_lda2:\n",
    "        vec2[topic] = prob\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = lda_similarity(text1, text2)\n",
    "    lda_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lda_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "class DirichletLanguageModel:\n",
    "    def __init__(self, documents, alpha=0.1):\n",
    "        self.alpha = alpha  # Dirichlet prior\n",
    "        self.token_counts = FreqDist()  # Total count of tokens\n",
    "        self.doc_lengths = []  # Length of each document\n",
    "        self.vocab_size = 0  # Size of the vocabulary\n",
    "        self.build_model(documents)\n",
    "\n",
    "    def build_model(self, documents):\n",
    "        for doc in documents:\n",
    "            tokens = nltk.word_tokenize(doc.lower())\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "            self.doc_lengths.append(len(filtered_tokens))\n",
    "            self.token_counts.update(filtered_tokens)\n",
    "        self.vocab_size = len(self.token_counts)\n",
    "\n",
    "    def get_probability(self, term):\n",
    "        \"\"\" Calculate probability of the term with Dirichlet smoothing \"\"\"\n",
    "        term_count = self.token_counts[term]\n",
    "        total_tokens = sum(self.doc_lengths)\n",
    "        return (term_count + self.alpha) / (total_tokens + self.alpha * self.vocab_size)\n",
    "\n",
    "    def get_document_probability(self, doc):\n",
    "        \"\"\" Calculate the probability of a document \"\"\"\n",
    "        prob = 0\n",
    "        doc_tokens = nltk.word_tokenize(doc.lower())\n",
    "        for token in doc_tokens:\n",
    "            prob += math.log(self.get_probability(token))\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Dirichlet_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    documents = [text1,text2]\n",
    "    model = DirichletLanguageModel(documents)\n",
    "    doc_to_test = text1\n",
    "    result = model.get_document_probability(doc_to_test)\n",
    "    Dirichlet_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(Dirichlet_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelenik-Mercer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class JelinekMercerSmoothing:\n",
    "    def __init__(self, lambda_=0.5):\n",
    "        self.lambda_ = lambda_\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "\n",
    "    def calculate_unigram_counts(self, text):\n",
    "        unigram_counts = {}\n",
    "        for word in text.split():\n",
    "            unigram_counts[word] = unigram_counts.get(word, 0) + 1\n",
    "        return unigram_counts\n",
    "\n",
    "    def calculate_bigram_counts(self, text):\n",
    "        bigram_counts = {}\n",
    "        tokens = text.split()\n",
    "        for i in range(1, len(tokens)):\n",
    "            bigram = (tokens[i-1], tokens[i])\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "        return bigram_counts\n",
    "\n",
    "    def unigram_probability(self, word):\n",
    "        return self.unigram_counts.get(word, 0) / sum(self.unigram_counts.values())\n",
    "\n",
    "    def bigram_probability(self, word, prev_word):\n",
    "        numerator = self.bigram_counts.get((prev_word, word), 0)\n",
    "        denominator = self.unigram_counts.get(prev_word, 0)\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator / denominator\n",
    "\n",
    "    def jelinek_mercer_probability(self, word, prev_word):\n",
    "        p_interp = self.lambda_ * self.bigram_probability(word, prev_word) + (1 - self.lambda_) * self.unigram_probability(word)\n",
    "        return p_interp\n",
    "\n",
    "    def text_similarity(self, text1, text2):\n",
    "        self.unigram_counts = self.calculate_unigram_counts(text1 + \" \" + text2)\n",
    "        self.bigram_counts = self.calculate_bigram_counts(text1 + \" \" + text2)\n",
    "        \n",
    "        tokens1 = text1.split()\n",
    "        tokens2 = text2.split()\n",
    "        \n",
    "        # Calculate similarity based on bigram probabilities\n",
    "        similarity = 1.0\n",
    "        for i in range(1, len(tokens1)):\n",
    "            prev_word = tokens1[i-1]\n",
    "            word = tokens1[i]\n",
    "            if prev_word in tokens2 and word in tokens2:\n",
    "                sim_word = min(self.jelinek_mercer_probability(word, prev_word), self.jelinek_mercer_probability(prev_word, word))\n",
    "                similarity *= sim_word\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "# Example usage\n",
    "jm_smoothing = JelinekMercerSmoothing()\n",
    "\n",
    "# text1 = \"the quick brown fox\"\n",
    "# text2 = \"jumps over the lazy dog\"\n",
    "\n",
    "# similarity = jm_smoothing.text_similarity(text1, text2)\n",
    "# print(\"Similarity between text1 and text2:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "jm_smoothing_dataset = []\n",
    "for i in range(len(csv_data)):\n",
    "    text1 = csv_data[i][0]\n",
    "    text2 = csv_data[i][1]\n",
    "    result = jm_smoothing.text_similarity(text1, text2)\n",
    "    jm_smoothing_dataset.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9.5367431640625e-07,\n",
       " 0.0006925207756232686,\n",
       " 0.002066115702479339,\n",
       " 0.001736111111111111,\n",
       " 5.689576695493856e-05,\n",
       " 3.0517578125e-05,\n",
       " 3.498542274052478e-05,\n",
       " 0.00041649312786339016,\n",
       " 0.0007716049382716049,\n",
       " 7.687892082496206e-06,\n",
       " 0.0008650519031141869,\n",
       " 0.0006510416666666666,\n",
       " 2.0246193715581467e-05,\n",
       " 1.2648397321575387e-06,\n",
       " 0.00011104969103508183,\n",
       " 1.0,\n",
       " 0.0011890606420927466,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0012755102040816326,\n",
       " 0.0007304601899196495,\n",
       " 1.0,\n",
       " 0.0030864197530864196,\n",
       " 7.224761580900885e-11,\n",
       " 0.0004938271604938272,\n",
       " 1.0973936899862826e-05,\n",
       " 0.0030864197530864196,\n",
       " 0.0011890606420927466,\n",
       " 0.0008163265306122448,\n",
       " 3.906250000000001e-07,\n",
       " 0.0014792899408284025,\n",
       " 0.0011111111111111111,\n",
       " 0.00010337923129380014,\n",
       " 1.0,\n",
       " 0.0022675736961451243,\n",
       " 0.001736111111111111,\n",
       " 0.0009765625,\n",
       " 0.0013717421124828531,\n",
       " 0.002066115702479339,\n",
       " 0.001736111111111111,\n",
       " 0.0014792899408284025,\n",
       " 3.0517578125e-05,\n",
       " 3.7037037037037037e-05,\n",
       " 0.0007716049382716049,\n",
       " 0.0011890606420927466,\n",
       " 0.0010405827263267429,\n",
       " 0.0011111111111111111,\n",
       " 0.002066115702479339,\n",
       " 4.002031965029711e-07,\n",
       " 1.0760517689271222e-08,\n",
       " 1.5832309420589215e-10,\n",
       " 7.812500000000002e-07,\n",
       " 2.4807257531880298e-08,\n",
       " 3.7057171270885955e-14,\n",
       " 4.3225653906885883e-07,\n",
       " 3.832847673204662e-08,\n",
       " 1.903968584518355e-08,\n",
       " 3.906250000000001e-07,\n",
       " 4.470348358154297e-08,\n",
       " 0.0008163265306122448,\n",
       " 5.8593750000000015e-08,\n",
       " 2.1947873799725652e-05,\n",
       " 0.0012755102040816326,\n",
       " 5.555555555555556e-05,\n",
       " 1.0,\n",
       " 2.332361516034985e-05,\n",
       " 1.0,\n",
       " 4.55539358600583e-05,\n",
       " 4.5369882495320336e-11,\n",
       " 2.9250020694389636e-07,\n",
       " 2.438652644413961e-07,\n",
       " 3.906250000000001e-07,\n",
       " 2.332361516034985e-05,\n",
       " 0.0008650519031141869,\n",
       " 0.038461538461538464,\n",
       " 1.2648397321575385e-06,\n",
       " 7.233796296296296e-05,\n",
       " 9.5367431640625e-07,\n",
       " 1.413865210574015e-06,\n",
       " 1.0,\n",
       " 0.0006250000000000001,\n",
       " 3.906250000000001e-07,\n",
       " 1.5625000000000004e-05,\n",
       " 0.0004938271604938272,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0016666666666666668,\n",
       " 0.0022675736961451243,\n",
       " 1.0,\n",
       " 0.0006250000000000001,\n",
       " 0.0009765625,\n",
       " 0.0008163265306122448,\n",
       " 0.0007716049382716049,\n",
       " 0.0008163265306122448,\n",
       " 0.023255813953488372,\n",
       " 7.313095916762147e-08,\n",
       " 0.038461538461538464,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0008163265306122448,\n",
       " 2.438652644413961e-07,\n",
       " 2.143347050754458e-05,\n",
       " 1.0,\n",
       " 0.0007716049382716049,\n",
       " 0.00043402777777777775,\n",
       " 0.0011111111111111111,\n",
       " 1.0,\n",
       " 0.0016,\n",
       " 7.535204475308642e-07,\n",
       " 0.0011111111111111111,\n",
       " 0.0006790402897238569,\n",
       " 2.0384788513183594e-05,\n",
       " 2.54427030327702e-05,\n",
       " 0.00047258979206049145,\n",
       " 0.0012755102040816326,\n",
       " 1.0,\n",
       " 1.5625000000000004e-05,\n",
       " 0.0011111111111111111,\n",
       " 0.0010405827263267429,\n",
       " 0.00027392257121986856,\n",
       " 0.0008650519031141869,\n",
       " 0.0025000000000000005,\n",
       " 0.0010405827263267429,\n",
       " 0.0012755102040816326,\n",
       " 1.0,\n",
       " 0.0007304601899196495,\n",
       " 0.0016,\n",
       " 0.00041649312786339016,\n",
       " 1.0,\n",
       " 1.0,\n",
       " 0.0007689350249903883,\n",
       " 0.001736111111111111,\n",
       " 2.54427030327702e-05,\n",
       " 0.0012755102040816326,\n",
       " 0.0011111111111111111,\n",
       " 0.0011890606420927466,\n",
       " 0.000594883997620464,\n",
       " 1.0,\n",
       " 2.54427030327702e-05,\n",
       " 6.944444444444445e-06,\n",
       " 0.0009765625,\n",
       " 0.0005668934240362811,\n",
       " 1.0,\n",
       " 0.0006574621959237343,\n",
       " 2.54427030327702e-05,\n",
       " 1.6242186154943957e-06,\n",
       " 0.0012755102040816326,\n",
       " 0.0006574621959237343,\n",
       " 1.7608940646130728e-05,\n",
       " 0.0016,\n",
       " 0.0007304601899196495,\n",
       " 0.0013717421124828531,\n",
       " 4.6296296296296296e-06,\n",
       " 1.0,\n",
       " 0.0016,\n",
       " 1.0,\n",
       " 0.0025000000000000005,\n",
       " 0.0007304601899196495,\n",
       " 0.0008163265306122448,\n",
       " 0.04,\n",
       " 0.0019132653061224487,\n",
       " 0.0008163265306122448,\n",
       " 0.0005408328826392644,\n",
       " 0.00046168051708217906,\n",
       " 0.000594883997620464,\n",
       " 0.0014792899408284025,\n",
       " 0.0008650519031141869,\n",
       " 0.0006250000000000001,\n",
       " 0.0009765625,\n",
       " 1.500671500474079e-07,\n",
       " 0.0007304601899196495,\n",
       " 0.0013774104683195593,\n",
       " 0.0006790402897238569,\n",
       " 3.961251175926107e-11,\n",
       " 0.0003305785123966942,\n",
       " 0.0009765625,\n",
       " 1.7999999999999997e-05,\n",
       " 1.0,\n",
       " 2.4000000000000003e-07,\n",
       " 0.0007716049382716049,\n",
       " 0.001736111111111111,\n",
       " 0.0011890606420927466,\n",
       " 0.001736111111111111,\n",
       " 4.387503104158446e-07,\n",
       " 1.0,\n",
       " 0.0006250000000000001,\n",
       " 4.0187757201646085e-06,\n",
       " 0.0009765625,\n",
       " 0.000594883997620464,\n",
       " 0.0006250000000000001,\n",
       " 0.0003429355281207133,\n",
       " 0.0013717421124828531,\n",
       " 0.034482758620689655,\n",
       " 5.080526342529086e-05,\n",
       " 0.0016,\n",
       " 2.845132292628526e-10,\n",
       " 0.0007716049382716049,\n",
       " 0.0009182736455463729,\n",
       " 0.0013717421124828531,\n",
       " 0.0007304601899196495,\n",
       " 0.0007716049382716049,\n",
       " 0.0006574621959237343,\n",
       " 0.0009765625,\n",
       " 1.0,\n",
       " 0.0007716049382716049,\n",
       " 0.000533997864008544,\n",
       " 0.0006858710562414266,\n",
       " 6.663890045814243e-07,\n",
       " 0.0009765625,\n",
       " 0.0018903591682419658,\n",
       " 4.3225653906885883e-07,\n",
       " 3.356718472021751e-05,\n",
       " 0.0006925207756232686,\n",
       " 1.0,\n",
       " 0.0007304601899196495,\n",
       " 3.0517578125e-05,\n",
       " 0.0011111111111111111,\n",
       " 0.0007716049382716049,\n",
       " 0.001736111111111111,\n",
       " 1.0,\n",
       " 0.0011111111111111111,\n",
       " 0.0010405827263267429,\n",
       " 0.00035599857600569594,\n",
       " 0.0012975778546712804,\n",
       " 0.0007716049382716049,\n",
       " 0.0013717421124828531,\n",
       " 0.000594883997620464,\n",
       " 1.0,\n",
       " 0.0009765625,\n",
       " 6.586541322068185e-07,\n",
       " 0.0027700831024930744,\n",
       " 1.886626334788132e-05,\n",
       " 0.0034602076124567475,\n",
       " 0.0009182736455463729,\n",
       " 0.0013717421124828531,\n",
       " 0.0007716049382716049,\n",
       " 7.303570472151487e-06,\n",
       " 0.0006574621959237343,\n",
       " 0.024390243902439025,\n",
       " 0.0010405827263267429,\n",
       " 0.0007716049382716049,\n",
       " 1.9742167295125664e-05,\n",
       " 0.0027700831024930744,\n",
       " 2.438652644413961e-07,\n",
       " 0.002066115702479339,\n",
       " 0.0034602076124567475,\n",
       " 2.7826474107465846e-05,\n",
       " 0.0018903591682419658,\n",
       " 0.0002465483234714004,\n",
       " 0.0011111111111111111]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jm_smoothing_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = list(zip(tfidf_dataset, BM25_dataset, Jensen_Shannon_dataset,\n",
    "                          lsa_dataset, lda_dataset, Dirichlet_dataset,\n",
    "                          jm_smoothing_dataset))\n",
    "\n",
    "\n",
    "merged_dataset = [list(item) for item in merged_dataset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(merged_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(merged_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(save_path, index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

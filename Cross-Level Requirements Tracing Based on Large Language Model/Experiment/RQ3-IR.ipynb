{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From E:\\anconda\\envs\\py310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.metrics import classification_report\n",
    "# from imblearn.over_sampling import SMOTE\n",
    "# from sklearn.feature_selection import VarianceThreshold\n",
    "from scipy.stats import pearsonr\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file_path = \"E:/jupyterCode/data/T2T/46/CCHIT.csv\"\n",
    "file_path = \"E:/jupyterCode/data/T2T/46/CM1Dataset.csv\"\n",
    "# save_path = \"E:/jupyterCode/data/T2T/46/IR_dataset/IR_CCHIT.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_csv_as_list(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', newline='', encoding='utf-8') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        first_row = True\n",
    "        for row in csv_reader:\n",
    "            if first_row:  \n",
    "                first_row = False\n",
    "                continue\n",
    "            data.append(row)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_data = read_csv_as_list(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_column=[]\n",
    "low_column=[]\n",
    "label_cloumn = []\n",
    "for i in range(0,len(csv_data)):\n",
    "    high_column.append(csv_data[i][0])\n",
    "    low_column.append(csv_data[i][1])\n",
    "    label_cloumn.append(csv_data[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# high_column = row[0] for row in csv_data\n",
    "# low_column = row[1] for row in csv_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "671"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(high_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vsm_count_key(text):\n",
    "    words = text.split()\n",
    "    table = {}\n",
    "    for word in words:\n",
    "        if word != \"\":\n",
    "            if word in table:\n",
    "                table[word] += 1\n",
    "            else:\n",
    "                table[word] = 1\n",
    "    return sorted(table.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "# 合并关键词及计算相似度\n",
    "def vsm_merge_keys(dic1, dic2):\n",
    "    arrayKey = []\n",
    "    for item in dic1:\n",
    "        arrayKey.append(item[0])\n",
    "    for item in dic2:\n",
    "        if item[0] not in arrayKey:\n",
    "            arrayKey.append(item[0])\n",
    "    \n",
    "    arrayNum1 = [0] * len(arrayKey)\n",
    "    arrayNum2 = [0] * len(arrayKey)\n",
    "    \n",
    "    for i, key in enumerate(arrayKey):\n",
    "        for k, v in dic1:\n",
    "            if key == k:\n",
    "                arrayNum1[i] = v\n",
    "        for k, v in dic2:\n",
    "            if key == k:\n",
    "                arrayNum2[i] = v\n",
    "    \n",
    "    # 计算两个向量的余弦相似度\n",
    "    dot_product = sum(a * b for a, b in zip(arrayNum1, arrayNum2))\n",
    "    magnitude1 = math.sqrt(sum(a * a for a in arrayNum1))\n",
    "    magnitude2 = math.sqrt(sum(b * b for b in arrayNum2))\n",
    "    if magnitude1 == 0 or magnitude2 == 0:\n",
    "        return 0\n",
    "    return dot_product / (magnitude1 * magnitude2)\n",
    "\n",
    "def compute_similarity(text1, text2):\n",
    "    # 统计每个文档的关键词及其个数\n",
    "    dic1 = vsm_count_key(text1)\n",
    "    dic2 = vsm_count_key(text2)\n",
    "    # 合并关键词及计算相似度\n",
    "    result = vsm_merge_keys(dic1, dic2)\n",
    "    similarity_matrix = np.array(result)\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = compute_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "    print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "#     return ranked_similarities\n",
    "\n",
    "# predic_list = compute_similarity_ranking(high_column, low_column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.5186\n",
      "精确率 (Precision): 0.8654\n",
      "召回率 (Recall): 0.1247\n",
      "F1分数 (F1 Score): 0.2179\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, f1_score\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Okapi BM25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import Counter\n",
    "\n",
    "def compute_idf(docs):\n",
    "    D = len(docs)\n",
    "    df = {}\n",
    "    for doc in docs:\n",
    "        for word in set(doc):\n",
    "            df[word] = df.get(word, 0) + 1\n",
    "    \n",
    "    idf = {}\n",
    "    for word, freq in df.items():\n",
    "        idf[word] = math.log((D - freq + 0.5) / (freq + 0.5) + 1)\n",
    "    \n",
    "    return idf\n",
    "\n",
    "def compute_score(doc1, doc2, idf):\n",
    "    avgdl = (len(doc1) + len(doc2)) / 2\n",
    "    k1 = 1.5\n",
    "    b = 0.75\n",
    "    score = 0\n",
    "    doc1_counter = Counter(doc1)\n",
    "    doc2_counter = Counter(doc2)\n",
    "    for word in set(doc1 + doc2):\n",
    "        if word not in idf:\n",
    "            continue\n",
    "        idf_val = idf[word]\n",
    "        tf1 = doc1_counter[word]\n",
    "        tf2 = doc2_counter[word]\n",
    "        doc1_len = len(doc1)\n",
    "        doc2_len = len(doc2)\n",
    "        score += (idf_val * ((tf1 * (k1 + 1)) / (tf1 + k1 * (1 - b + b * doc1_len / avgdl))) *\n",
    "                              ((tf2 * (k1 + 1)) / (tf2 + k1 * (1 - b + b * doc2_len / avgdl))))\n",
    "    return score\n",
    "\n",
    "def compute_bm25_similarity(doc1, doc2):\n",
    "    idf = compute_idf([doc1, doc2])\n",
    "    score = compute_score(doc1, doc2, idf)\n",
    "    return score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.4575\n",
      "精确率 (Precision): 0.4000\n",
      "召回率 (Recall): 0.0166\n",
      "F1分数 (F1 Score): 0.0319\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = compute_bm25_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jensen Shannon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def count_words(text):\n",
    "    words = text.split()\n",
    "    return Counter(words)\n",
    "\n",
    "def compute_js_similarity(text1, text2):\n",
    "    counter1 = count_words(text1)\n",
    "    counter2 = count_words(text2)\n",
    "    \n",
    "    \n",
    "    all_words = list(set(counter1.keys()).union(set(counter2.keys())))\n",
    "    \n",
    "   \n",
    "    vector1 = np.array([counter1.get(word, 0) for word in all_words], dtype=float)\n",
    "    vector2 = np.array([counter2.get(word, 0) for word in all_words], dtype=float)\n",
    "    \n",
    "   \n",
    "    vector1 /= vector1.sum()\n",
    "    vector2 /= vector2.sum()\n",
    "    \n",
    "    \n",
    "    js_distance = jensenshannon(vector1, vector2)\n",
    "    js_similarity = 1 - js_distance\n",
    "    return js_similarity\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.5440\n",
      "精确率 (Precision): 0.9231\n",
      "召回率 (Recall): 0.1662\n",
      "F1分数 (F1 Score): 0.2817\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = compute_js_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Semantic Analysis (LSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "def cosine_lsa_similarity_measure(text1, text2, n=1):\n",
    "    \n",
    "    vectorizer = CountVectorizer()\n",
    "    X = vectorizer.fit_transform([text1, text2])\n",
    "    \n",
    "    \n",
    "    svd_model = TruncatedSVD(n_components=n)  \n",
    "    X2 = svd_model.fit_transform(X)  \n",
    "    cosine_sim = cosine_similarity(X)\n",
    "    return cosine_sim[0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.5499\n",
      "精确率 (Precision): 0.9275\n",
      "召回率 (Recall): 0.1773\n",
      "F1分数 (F1 Score): 0.2977\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = cosine_lsa_similarity_measure(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Latent Dirichlet Allocation (LDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models import LdaModel\n",
    "\n",
    "# Download necessary resources\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\" Preprocess text: lowercase, tokenize, remove stopwords. \"\"\"\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    words = [w.lower() for w in nltk.word_tokenize(text) if w.lower() not in stop_words]\n",
    "    return words\n",
    "\n",
    "def lda_similarity(text1, text2):\n",
    "    \"\"\" Calculate LDA similarity between two text strings. \"\"\"\n",
    "    processed_text1 = preprocess_text(text1)\n",
    "    processed_text2 = preprocess_text(text2)\n",
    "    \n",
    "    # Create dictionary and corpus\n",
    "    dictionary = Dictionary([processed_text1, processed_text2])\n",
    "    corpus = [dictionary.doc2bow(processed_text1), dictionary.doc2bow(processed_text2)]\n",
    "    \n",
    "    # Train LDA model\n",
    "    lda_model = LdaModel(corpus=corpus, id2word=dictionary, num_topics=2)\n",
    "    \n",
    "    # Get document topics\n",
    "    doc_bow1 = dictionary.doc2bow(processed_text1)\n",
    "    doc_bow2 = dictionary.doc2bow(processed_text2)\n",
    "    \n",
    "    doc_lda1 = lda_model.get_document_topics(doc_bow1)\n",
    "    doc_lda2 = lda_model.get_document_topics(doc_bow2)\n",
    "    \n",
    "    # Create dense topic distribution vectors\n",
    "    vec1 = np.zeros(lda_model.num_topics)\n",
    "    vec2 = np.zeros(lda_model.num_topics)\n",
    "    for topic, prob in doc_lda1:\n",
    "        vec1[topic] = prob\n",
    "    for topic, prob in doc_lda2:\n",
    "        vec2[topic] = prob\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    if np.all(vec1 == 0) or np.all(vec2 == 0):\n",
    "        return 0\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.4605\n",
      "精确率 (Precision): 0.4444\n",
      "召回率 (Recall): 0.0111\n",
      "F1分数 (F1 Score): 0.0216\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []        \n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = lda_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dirichlet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.probability import FreqDist, MLEProbDist\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "\n",
    "class DirichletLanguageModel:\n",
    "    def __init__(self, documents, alpha=0.1):\n",
    "        self.alpha = alpha  # Dirichlet prior\n",
    "        self.token_counts = FreqDist()  # Total count of tokens\n",
    "        self.doc_lengths = []  # Length of each document\n",
    "        self.vocab_size = 0  # Size of the vocabulary\n",
    "        self.build_model(documents)\n",
    "\n",
    "    def build_model(self, documents):\n",
    "        for doc in documents:\n",
    "            tokens = nltk.word_tokenize(doc.lower())\n",
    "            filtered_tokens = [token for token in tokens if token not in stopwords.words('english')]\n",
    "            self.doc_lengths.append(len(filtered_tokens))\n",
    "            self.token_counts.update(filtered_tokens)\n",
    "        self.vocab_size = len(self.token_counts)\n",
    "\n",
    "    def get_probability(self, term):\n",
    "        \"\"\" Calculate probability of the term with Dirichlet smoothing \"\"\"\n",
    "        term_count = self.token_counts[term]\n",
    "        total_tokens = sum(self.doc_lengths)\n",
    "        return (term_count + self.alpha) / (total_tokens + self.alpha * self.vocab_size)\n",
    "\n",
    "    def get_document_probability(self, doc):\n",
    "        \"\"\" Calculate the probability of a document \"\"\"\n",
    "        prob = 0\n",
    "        doc_tokens = nltk.word_tokenize(doc.lower())\n",
    "        for token in doc_tokens:\n",
    "            prob += math.log(self.get_probability(token))\n",
    "        return prob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.4858\n",
      "精确率 (Precision): 0.9000\n",
      "召回率 (Recall): 0.0499\n",
      "F1分数 (F1 Score): 0.0945\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            documents = [text1,text2]\n",
    "            model = DirichletLanguageModel(documents)\n",
    "            doc_to_test = text1\n",
    "            sim = model.get_document_probability(doc_to_test)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jelenik-Mercer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class JelinekMercerSmoothing:\n",
    "    def __init__(self, lambda_=0.5):\n",
    "        self.lambda_ = lambda_\n",
    "        self.unigram_counts = {}\n",
    "        self.bigram_counts = {}\n",
    "\n",
    "    def calculate_unigram_counts(self, text):\n",
    "        unigram_counts = {}\n",
    "        for word in text.split():\n",
    "            unigram_counts[word] = unigram_counts.get(word, 0) + 1\n",
    "        return unigram_counts\n",
    "\n",
    "    def calculate_bigram_counts(self, text):\n",
    "        bigram_counts = {}\n",
    "        tokens = text.split()\n",
    "        for i in range(1, len(tokens)):\n",
    "            bigram = (tokens[i-1], tokens[i])\n",
    "            bigram_counts[bigram] = bigram_counts.get(bigram, 0) + 1\n",
    "        return bigram_counts\n",
    "\n",
    "    def unigram_probability(self, word):\n",
    "        return self.unigram_counts.get(word, 0) / sum(self.unigram_counts.values())\n",
    "\n",
    "    def bigram_probability(self, word, prev_word):\n",
    "        numerator = self.bigram_counts.get((prev_word, word), 0)\n",
    "        denominator = self.unigram_counts.get(prev_word, 0)\n",
    "        if denominator == 0:\n",
    "            return 0\n",
    "        else:\n",
    "            return numerator / denominator\n",
    "\n",
    "    def jelinek_mercer_probability(self, word, prev_word):\n",
    "        p_interp = self.lambda_ * self.bigram_probability(word, prev_word) + (1 - self.lambda_) * self.unigram_probability(word)\n",
    "        return p_interp\n",
    "\n",
    "    def text_similarity(self, text1, text2):\n",
    "        self.unigram_counts = self.calculate_unigram_counts(text1 + \" \" + text2)\n",
    "        self.bigram_counts = self.calculate_bigram_counts(text1 + \" \" + text2)\n",
    "        \n",
    "        tokens1 = text1.split()\n",
    "        tokens2 = text2.split()\n",
    "        \n",
    "        # Calculate similarity based on bigram probabilities\n",
    "        similarity = 1.0\n",
    "        for i in range(1, len(tokens1)):\n",
    "            prev_word = tokens1[i-1]\n",
    "            word = tokens1[i]\n",
    "            if prev_word in tokens2 and word in tokens2:\n",
    "                sim_word = min(self.jelinek_mercer_probability(word, prev_word), self.jelinek_mercer_probability(prev_word, word))\n",
    "                similarity *= sim_word\n",
    "        \n",
    "        return similarity\n",
    "\n",
    "# Example usage\n",
    "jm_smoothing = JelinekMercerSmoothing()\n",
    "\n",
    "# text1 = \"the quick brown fox\"\n",
    "# text2 = \"jumps over the lazy dog\"\n",
    "\n",
    "# similarity = jm_smoothing.text_similarity(text1, text2)\n",
    "# print(\"Similarity between text1 and text2:\", similarity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.4531\n",
      "精确率 (Precision): 0.1250\n",
      "召回率 (Recall): 0.0028\n",
      "F1分数 (F1 Score): 0.0054\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = jm_smoothing.text_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import corpora, models, similarities\n",
    "\n",
    "def build_lsi_model(documents, num_topics=2):\n",
    "    texts = [[word for word in document.lower().split()] for document in documents]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus)\n",
    "    corpus_tfidf = tfidf[corpus]\n",
    "    lsi = models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=num_topics)\n",
    "    return lsi, dictionary, corpus\n",
    "\n",
    "def compute_lsi_similarity(str1, str2):\n",
    "   \n",
    "    documents = [str1, str2]\n",
    "    \n",
    "    lsi_model, dictionary, corpus = build_lsi_model(documents)\n",
    "    \n",
    "   \n",
    "    vec_bow1 = dictionary.doc2bow(str1.lower().split())\n",
    "    vec_bow2 = dictionary.doc2bow(str2.lower().split())\n",
    "    \n",
    "    vec_lsi1 = lsi_model[vec_bow1]\n",
    "    vec_lsi2 = lsi_model[vec_bow2]\n",
    "    \n",
    "  \n",
    "    index = similarities.MatrixSimilarity([vec_lsi1], num_features=lsi_model.num_topics)\n",
    "    sims = index[vec_lsi2]\n",
    "    \n",
    "    return sims[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率 (Accuracy): 0.4426\n",
      "精确率 (Precision): 0.0667\n",
      "召回率 (Recall): 0.0028\n",
      "F1分数 (F1 Score): 0.0053\n"
     ]
    }
   ],
   "source": [
    "def compute_similarity_ranking(high_column, low_column):\n",
    "#     similarity_matrix = []\n",
    "    rankings = []\n",
    "#     for text1 in high_column:\n",
    "    for i in range(0,len(high_column)):\n",
    "        text1 = high_column[i]\n",
    "        similarities = []\n",
    "        for text2 in low_column:\n",
    "            sim = compute_lsi_similarity(text1, text2)\n",
    "            similarities.append(sim)\n",
    "            \n",
    "        similarities = np.array(similarities)\n",
    "        rank_order = similarities.argsort()[::-1].argsort() + 1\n",
    "        rankings.append(rank_order[i])\n",
    "#     print(rankings)\n",
    "    predic_list = []\n",
    "    for item in rankings:\n",
    "        label=1\n",
    "        if item > 15:\n",
    "            label=0\n",
    "        predic_list.append(label)\n",
    "#     print(predic_list)\n",
    "            \n",
    "    return predic_list\n",
    "\n",
    "predic_list = compute_similarity_ranking(high_column, low_column)\n",
    "list1 = [int(x) for x in label_cloumn]\n",
    "list2 = [int(x) for x in predic_list]\n",
    "precision = precision_score(list1, list2)\n",
    "accuracy = accuracy_score(list1, list2)\n",
    "recall = recall_score(list1, list2)\n",
    "f1 = f1_score(list1, list2)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "py310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
